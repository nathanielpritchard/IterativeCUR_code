
@article{deshpande2006no,
	title = {Matrix {Approximation} and {Projective} {Clustering} {Via} {Volume} {Sampling}},
	volume = {2},
	issn = {1557-2862},
	url = {https://theoryofcomputing.org/articles/v002a012},
	doi = {10.4086/toc.2006.v002a012},
	language = {en},
	number = {1},
	urldate = {2025-04-24},
	journal = {Theory of Computing},
	author = {Deshpande, Amit and Rademacher, Luis and Vempala, Santosh S. and Wang, Grant},
	year = {2006},
	pages = {225--247},
}

@article{cortinovis2020lowrank,
	title = {Low-{Rank} {Approximation} in the {Frobenius} {Norm} by {Column} and {Row} {Subset} {Selection}},
	volume = {41},
	issn = {0895-4798, 1095-7162},
	url = {https://epubs.siam.org/doi/10.1137/19M1281848},
	doi = {10.1137/19M1281848},
	language = {en},
	number = {4},
	urldate = {2025-09-12},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Cortinovis, Alice and Kressner, Daniel},
	month = jan,
	year = {2020},
	pages = {1651--1673},
}

@article{osinsky2025close,
	title = {Close to optimal column approximation using a single {SVD}},
	volume = {725},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {0024-3795},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0024379525003003},
	doi = {10.1016/j.laa.2025.07.016},
	language = {en},
	urldate = {2025-07-24},
	journal = {Linear Algebra and its Applications},
	author = {Osinsky, A.I.},
	month = nov,
	year = {2025},
	note = {Publisher: Elsevier BV},
	pages = {359--377},
}

@article{davis2011university,
	title = {The university of {Florida} sparse matrix collection},
	volume = {38},
	issn = {0098-3500, 1557-7295},
	url = {https://dl.acm.org/doi/10.1145/2049662.2049663},
	doi = {10.1145/2049662.2049663},
	abstract = {We describe the University of Florida Sparse Matrix Collection, a large and actively growing set of sparse matrices that arise in real applications. The Collection is widely used by the numerical linear algebra community for the development and performance evaluation of sparse matrix algorithms. It allows for robust and repeatable experiments: robust because performance results with artificially generated matrices can be misleading, and repeatable because matrices are curated and made publicly available in many formats. Its matrices cover a wide spectrum of domains, include those arising from problems with underlying 2D or 3D geometry (as structural engineering, computational fluid dynamics, model reduction, electromagnetics, semiconductor devices, thermodynamics, materials, acoustics, computer graphics/vision, robotics/kinematics, and other discretizations) and those that typically do not have such geometry (optimization, circuit simulation, economic and financial modeling, theoretical and quantum chemistry, chemical process simulation, mathematics and statistics, power networks, and other networks and graphs). We provide software for accessing and managing the Collection, from MATLAB™, Mathematica™, Fortran, and C, as well as an online search capability. Graph visualization of the matrices is provided, and a new multilevel coarsening scheme is proposed to facilitate this task.},
	language = {en},
	number = {1},
	urldate = {2025-07-05},
	journal = {ACM Transactions on Mathematical Software},
	author = {Davis, Timothy A. and Hu, Yifan},
	month = nov,
	year = {2011},
	pages = {1--25},
}

@article{tropp2011improved,
	title = {{IMPROVED} {ANALYSIS} {OF} {THE} {SUBSAMPLED} {RANDOMIZED} {HADAMARD} {TRANSFORM}},
	volume = {03},
	issn = {1793-5369, 1793-7175},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S1793536911000787},
	doi = {10.1142/S1793536911000787},
	abstract = {This paper presents an improved analysis of a structured dimension-reduction map called the subsampled randomized Hadamard transform. This argument demonstrates that the map preserves the Euclidean geometry of an entire subspace of vectors. The new proof is much simpler than previous approaches, and it offers — for the first time — optimal constants in the estimate on the number of dimensions required for the embedding.},
	language = {en},
	number = {01n02},
	urldate = {2025-07-03},
	journal = {Advances in Adaptive Data Analysis},
	author = {Tropp, Joel A.},
	month = apr,
	year = {2011},
	pages = {115--126},
}

@article{chaturantabut2010nonlinear,
	title = {Nonlinear {Model} {Reduction} via {Discrete} {Empirical} {Interpolation}},
	volume = {32},
	issn = {1064-8275, 1095-7197},
	url = {http://epubs.siam.org/doi/10.1137/090766498},
	doi = {10.1137/090766498},
	language = {en},
	number = {5},
	urldate = {2025-06-30},
	journal = {SIAM Journal on Scientific Computing},
	author = {Chaturantabut, Saifon and Sorensen, Danny C.},
	month = jan,
	year = {2010},
	pages = {2737--2764},
}

@article{park2025accuracy,
	title = {Accuracy and {Stability} of {CUR} {Decompositions} with {Oversampling}},
	volume = {46},
	issn = {0895-4798, 1095-7162},
	url = {https://epubs.siam.org/doi/10.1137/24M1660346},
	doi = {10.1137/24M1660346},
	language = {en},
	number = {1},
	urldate = {2025-06-30},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Park, Taejun and Nakatsukasa, Yuji},
	month = mar,
	year = {2025},
	pages = {780--810},
}

@article{shitov2021column,
	title = {Column subset selection is {NP}-complete},
	volume = {610},
	issn = {00243795},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0024379520304377},
	doi = {10.1016/j.laa.2020.09.015},
	language = {en},
	urldate = {2025-06-29},
	journal = {Linear Algebra and its Applications},
	author = {Shitov, Yaroslav},
	month = feb,
	year = {2021},
	pages = {52--58},
}

@article{goreinov1997theory,
	title = {A theory of pseudoskeleton approximations},
	volume = {261},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00243795},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0024379596003011},
	doi = {10.1016/S0024-3795(96)00301-1},
	language = {en},
	number = {1-3},
	urldate = {2025-06-29},
	journal = {Linear Algebra and its Applications},
	author = {Goreinov, S.A. and Tyrtyshnikov, E.E. and Zamarashkin, N.L.},
	month = aug,
	year = {1997},
	pages = {1--21},
}

@article{mirsky1960symmetric,
	title = {{SYMMETRIC} {GAUGE} {FUNCTIONS} {AND} {UNITARILY} {INVARIANT} {NORMS}},
	volume = {11},
	issn = {0033-5606, 1464-3847},
	url = {https://academic.oup.com/qjmath/article-lookup/doi/10.1093/qmath/11.1.50},
	doi = {10.1093/qmath/11.1.50},
	language = {en},
	number = {1},
	urldate = {2025-06-29},
	journal = {The Quarterly Journal of Mathematics},
	author = {Mirsky, L.},
	year = {1960},
	pages = {50--59},
}

@article{park2025lowrank,
	title = {Low-{Rank} {Approximation} of {Parameter}-{Dependent} {Matrices} via {CUR} {Decomposition}},
	volume = {47},
	issn = {1064-8275, 1095-7197},
	url = {https://epubs.siam.org/doi/10.1137/24M1683998},
	doi = {10.1137/24M1683998},
	language = {en},
	number = {3},
	urldate = {2025-05-30},
	journal = {SIAM Journal on Scientific Computing},
	author = {Park, Taejun and Nakatsukasa, Yuji},
	month = jun,
	year = {2025},
	pages = {A1858--A1887},
}

@article{gratton2018improved,
	title = {Improved {Bounds} for {Small}-{Sample} {Estimation}},
	volume = {39},
	issn = {0895-4798, 1095-7162},
	url = {https://epubs.siam.org/doi/10.1137/17M1137541},
	doi = {10.1137/17M1137541},
	language = {en},
	number = {2},
	urldate = {2025-05-27},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Gratton, Serge and Titley-Peloquin, David},
	month = jan,
	year = {2018},
	pages = {922--931},
}

@article{pearce2025adaptive,
	title = {Adaptive {Parallelizable} {Algorithms} for {Interpolative} {Decompositions} via {Partially} {Pivoted} {LU}},
	volume = {32},
	issn = {1070-5325, 1099-1506},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/nla.70002},
	doi = {10.1002/nla.70002},
	abstract = {ABSTRACT
            Interpolative decompositions (ID) involve “natural bases” of row and column subsets, or skeletons, of a given matrix that approximately span its row and column spaces. Although finding optimal skeleton subsets is combinatorially hard, classical greedy pivoting algorithms with rank‐revealing properties like column‐pivoted QR (CPQR) often provide good heuristics in practice. To select skeletons efficiently for large matrices, randomized sketching is commonly leveraged as a preprocessing step to reduce the problem dimension while preserving essential information in the matrix. In addition to accelerating computations, randomization via sketching improves robustness against adversarial inputs while relaxing the rank‐revealing assumption on the pivoting scheme. This enables faster skeleton selection based on LU with partial pivoting (LUPP) as a reliable alternative to rank‐revealing pivoting methods like CPQR. However, while coupling sketching with LUPP provides an efficient solution for ID with a given rank, the lack of rank‐revealing properties of LUPP makes it challenging to adaptively determine a suitable rank without prior knowledge of the matrix spectrum. As a remedy, in this work, we introduce an adaptive randomized LUPP algorithm that approximates the desired rank via fast estimation of the residual error. The resulting algorithm is not only adaptive but also parallelizable, attaining much higher practical speed due to the lower communication requirements of LUPP over CPQR. The method has been implemented for both CPUs and GPUs, and the resulting software has been made publicly available.},
	language = {en},
	number = {1},
	urldate = {2025-05-08},
	journal = {Numerical Linear Algebra with Applications},
	author = {Pearce, Katherine and Chen, Chao and Dong, Yijun and Martinsson, Per‐Gunnar},
	month = feb,
	year = {2025},
	pages = {e70002},
}

@article{tropp2019streaming,
	title = {Streaming {Low}-{Rank} {Matrix} {Approximation} with an {Application} to {Scientific} {Simulation}},
	volume = {41},
	issn = {1064-8275, 1095-7197},
	url = {https://epubs.siam.org/doi/10.1137/18M1201068},
	doi = {10.1137/18M1201068},
	language = {en},
	number = {4},
	urldate = {2025-04-28},
	journal = {SIAM Journal on Scientific Computing},
	author = {Tropp, Joel A. and Yurtsever, Alp and Udell, Madeleine and Cevher, Volkan},
	month = jan,
	year = {2019},
	pages = {A2430--A2463},
}

@inproceedings{cohen2016nearly,
	title = {Nearly {Tight} {Oblivious} {Subspace} {Embeddings} by {Trace} {Inequalities}},
	isbn = {978-1-61197-433-1},
	url = {http://epubs.siam.org/doi/10.1137/1.9781611974331.ch21},
	doi = {10.1137/1.9781611974331.ch21},
	language = {en},
	urldate = {2025-04-28},
	booktitle = {Proceedings of the {Twenty}-{Seventh} {Annual} {ACM}-{SIAM} {Symposium} on {Discrete} {Algorithms}},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Cohen, Michael B.},
	month = jan,
	year = {2016},
	pages = {278--287},
}

@incollection{deshpande2006adaptive,
	address = {Berlin, Heidelberg},
	title = {Adaptive {Sampling} and {Fast} {Low}-{Rank} {Matrix} {Approximation}},
	volume = {4110},
	isbn = {978-3-540-38044-3 978-3-540-38045-0},
	url = {http://link.springer.com/10.1007/11830924_28},
	urldate = {2025-04-24},
	booktitle = {Approximation, {Randomization}, and {Combinatorial} {Optimization}. {Algorithms} and {Techniques}},
	publisher = {Springer Berlin Heidelberg},
	author = {Deshpande, Amit and Vempala, Santosh},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Díaz, Josep and Jansen, Klaus and Rolim, José D. P. and Zwick, Uri},
	year = {2006},
	doi = {10.1007/11830924_28},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {292--303},
}

@article{demmel2015communication,
	title = {Communication {Avoiding} {Rank} {Revealing} {QR} {Factorization} with {Column} {Pivoting}},
	volume = {36},
	issn = {0895-4798, 1095-7162},
	url = {http://epubs.siam.org/doi/10.1137/13092157X},
	doi = {10.1137/13092157X},
	language = {en},
	number = {1},
	urldate = {2025-04-24},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Demmel, James W. and Grigori, Laura and Gu, Ming and Xiang, Hua},
	month = jan,
	year = {2015},
	pages = {55--89},
}

@article{martinsson2017householder,
	title = {Householder {QR} {Factorization} {With} {Randomization} for {Column} {Pivoting} ({HQRRP})},
	volume = {39},
	issn = {1064-8275, 1095-7197},
	url = {https://epubs.siam.org/doi/10.1137/16M1081270},
	doi = {10.1137/16M1081270},
	language = {en},
	number = {2},
	urldate = {2025-04-23},
	journal = {SIAM Journal on Scientific Computing},
	author = {Martinsson, Per-Gunnar and Quintana OrtÍ, Gregorio and Heavner, Nathan and Van De Geijn, Robert},
	month = jan,
	year = {2017},
	pages = {C96--C115},
}

@article{chen2025randomly,
	title = {Randomly pivoted {Cholesky}: {Practical} approximation of a kernel matrix with few entry evaluations},
	volume = {78},
	issn = {0010-3640, 1097-0312},
	shorttitle = {Randomly pivoted {Cholesky}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/cpa.22234},
	doi = {10.1002/cpa.22234},
	abstract = {Abstract
            
              The randomly pivoted Cholesky algorithm (
              RPCholesky
              ) computes a factorized rank‐ approximation of an  positive‐semidefinite (psd) matrix.
              RPCholesky
              requires only  entry evaluations and  additional arithmetic operations, and it can be implemented with just a few lines of code. The method is particularly useful for approximating a kernel matrix. This paper offers a thorough new investigation of the empirical and theoretical behavior of this fundamental algorithm. For matrix approximation problems that arise in scientific machine learning, experiments show that
              RPCholesky
              matches or beats the performance of alternative algorithms. Moreover,
              RPCholesky
              provably returns low‐rank approximations that are nearly optimal. The simplicity, effectiveness, and robustness of
              RPCholesky
              strongly support its use in scientific computing and machine learning applications.},
	language = {en},
	number = {5},
	urldate = {2025-04-23},
	journal = {Communications on Pure and Applied Mathematics},
	author = {Chen, Yifan and Epperly, Ethan N. and Tropp, Joel A. and Webber, Robert J.},
	month = may,
	year = {2025},
	pages = {995--1041},
}

@article{eckart1936approximation,
	title = {The {Approximation} of {One} {Matrix} by {Another} of {Lower} {Rank}},
	volume = {1},
	copyright = {https://www.cambridge.org/core/terms},
	issn = {0033-3123, 1860-0980},
	url = {https://www.cambridge.org/core/product/identifier/S0033312300051085/type/journal_article},
	doi = {10.1007/BF02288367},
	abstract = {The mathematical problem of approximating one matrix by another of lower rank is closely related to the fundamental postulate of factor-theory. When formulated as a least-squares problem, the normal equations cannot be immediately written down, since the elements of the approximate matrix are not independent of one another. The solution of the problem is simplified by first expressing the matrices in a canonic form. It is found that the problem always has a solution which is usually unique. Several conclusions can be drawn from the form of this solution.
            A hypothetical interpretation of the canonic components of a score matrix is discussed.},
	language = {en},
	number = {3},
	urldate = {2025-04-18},
	journal = {Psychometrika},
	author = {Eckart, Carl and Young, Gale},
	month = sep,
	year = {1936},
	pages = {211--218},
}

@inproceedings{meng2013lowdistortion,
	address = {Palo Alto California USA},
	title = {Low-distortion subspace embeddings in input-sparsity time and applications to robust linear regression},
	isbn = {978-1-4503-2029-0},
	url = {https://dl.acm.org/doi/10.1145/2488608.2488621},
	doi = {10.1145/2488608.2488621},
	language = {en},
	urldate = {2025-04-17},
	booktitle = {Proceedings of the forty-fifth annual {ACM} symposium on {Theory} of {Computing}},
	publisher = {ACM},
	author = {Meng, Xiangrui and Mahoney, Michael W.},
	month = jun,
	year = {2013},
	pages = {91--100},
}

@inproceedings{indyk1998approximate,
	address = {Dallas, Texas, United States},
	title = {Approximate nearest neighbors: towards removing the curse of dimensionality},
	isbn = {978-0-89791-962-3},
	shorttitle = {Approximate nearest neighbors},
	url = {http://portal.acm.org/citation.cfm?doid=276698.276876},
	doi = {10.1145/276698.276876},
	language = {en},
	urldate = {2025-04-17},
	booktitle = {Proceedings of the thirtieth annual {ACM} symposium on {Theory} of computing  - {STOC} '98},
	publisher = {ACM Press},
	author = {Indyk, Piotr and Motwani, Rajeev},
	year = {1998},
	pages = {604--613},
}

@article{ailon2009fast,
	title = {The {Fast} {Johnson}–{Lindenstrauss} {Transform} and {Approximate} {Nearest} {Neighbors}},
	volume = {39},
	issn = {0097-5397, 1095-7111},
	url = {http://epubs.siam.org/doi/10.1137/060673096},
	doi = {10.1137/060673096},
	language = {en},
	number = {1},
	urldate = {2025-04-17},
	journal = {SIAM Journal on Computing},
	author = {Ailon, Nir and Chazelle, Bernard},
	month = jan,
	year = {2009},
	pages = {302--322},
}

@article{woolfe2008fast,
	title = {A fast randomized algorithm for the approximation of matrices},
	volume = {25},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {10635203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1063520307001364},
	doi = {10.1016/j.acha.2007.12.002},
	language = {en},
	number = {3},
	urldate = {2025-04-17},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Woolfe, Franco and Liberty, Edo and Rokhlin, Vladimir and Tygert, Mark},
	month = nov,
	year = {2008},
	pages = {335--366},
}

@article{belabbas2009spectral,
	title = {Spectral methods in machine learning and new strategies for very large datasets},
	volume = {106},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.0810600105},
	doi = {10.1073/pnas.0810600105},
	abstract = {Spectral methods are of fundamental importance in statistics and machine learning, because they underlie algorithms from classical principal components analysis to more recent approaches that exploit manifold structure. In most cases, the core technical problem can be reduced to computing a low-rank approximation to a positive-definite kernel. For the growing number of applications dealing with very large or high-dimensional datasets, however, the optimal approximation afforded by an exact spectral decomposition is too costly, because its complexity scales as the cube of either the number of training examples or their dimensionality. Motivated by such applications, we present here 2 new algorithms for the approximation of positive-semidefinite kernels, together with error bounds that improve on results in the literature. We approach this problem by seeking to determine, in an efficient manner, the most informative subset of our data relative to the kernel approximation task at hand. This leads to two new strategies based on the Nyström method that are directly applicable to massive datasets. The first of these—based on sampling—leads to a randomized algorithm whereupon the kernel induces a probability distribution on its set of partitions, whereas the latter approach—based on sorting—provides for the selection of a partition in a deterministic way. We detail their numerical implementation and provide simulation results for a variety of representative problems in statistical data analysis, each of which demonstrates the improved performance of our approach relative to existing methods.},
	language = {en},
	number = {2},
	urldate = {2025-04-17},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Belabbas, Mohamed-Ali and Wolfe, Patrick J.},
	month = jan,
	year = {2009},
	pages = {369--374},
}

@article{derezinski2021determinantal,
	title = {Determinantal {Point} {Processes} in {Randomized} {Numerical} {Linear} {Algebra}},
	volume = {68},
	issn = {0002-9920, 1088-9477},
	url = {https://www.ams.org/notices/202101/rnoti-p34.pdf},
	doi = {10.1090/noti2202},
	language = {en},
	number = {01},
	urldate = {2025-04-17},
	journal = {Notices of the American Mathematical Society},
	author = {Dereziński, Michał and Mahoney, Michael W.},
	month = jan,
	year = {2021},
	pages = {1},
}

@article{frieze2004fast,
	title = {Fast monte-carlo algorithms for finding low-rank approximations},
	volume = {51},
	issn = {0004-5411, 1557-735X},
	url = {https://dl.acm.org/doi/10.1145/1039488.1039494},
	doi = {10.1145/1039488.1039494},
	abstract = {We consider the problem of approximating a given
              m
              ×
              n
              matrix
              A
              by another matrix of specified rank
              k
              , which is smaller than
              m
              and
              n
              . The Singular Value Decomposition (SVD) can be used to find the "best" such approximation. However, it takes time polynomial in
              m, n
              which is prohibitive for some modern applications. In this article, we develop an algorithm that is qualitatively faster, provided we may sample the entries of the matrix in accordance with a natural probability distribution. In many applications, such sampling can be done efficiently. Our main result is a randomized algorithm to find
              the description of
              a matrix
              
                D
              
              *
              of rank at most
              k
              so that holds with probability at least 1 − δ (where {\textbar}·{\textbar}
              
                F
              
              is the Frobenius norm). The algorithm takes time polynomial in
              k
              ,1/ϵ, log(1/δ) only and is
              independent of m and n
              . In particular, this implies that in constant time, it can be determined if a given matrix of arbitrary size has a good low-rank approximation.},
	language = {en},
	number = {6},
	urldate = {2025-04-17},
	journal = {Journal of the ACM},
	author = {Frieze, Alan and Kannan, Ravi and Vempala, Santosh},
	month = nov,
	year = {2004},
	pages = {1025--1041},
}

@article{meier2024fasta,
	title = {Fast randomized numerical rank estimation for numerically low-rank matrices},
	volume = {686},
	issn = {00243795},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0024379524000016},
	doi = {10.1016/j.laa.2024.01.001},
	language = {en},
	urldate = {2025-04-16},
	journal = {Linear Algebra and its Applications},
	author = {Meier, Maike and Nakatsukasa, Yuji},
	month = apr,
	year = {2024},
	pages = {1--32},
}

@misc{flynn2024stat,
	title = {{STAT}: {Shrinking} {Transformers} {After} {Training}},
	shorttitle = {{STAT}},
	url = {http://arxiv.org/abs/2406.00061},
	doi = {10.48550/arXiv.2406.00061},
	abstract = {We present STAT: a simple algorithm to prune transformer models without any fine-tuning. STAT eliminates both attention heads and neurons from the network, while preserving accuracy by calculating a correction to the weights of the next layer. Each layer block in the network is compressed using a series of principled matrix factorizations that preserve the network structure. Our entire algorithm takes minutes to compress BERT, and less than three hours to compress models with 7B parameters using a single GPU. Using only several hundred data examples, STAT preserves the output of the network and improves upon existing gradient-free pruning methods. It is even competitive with methods that include significant fine-tuning. We demonstrate our method on both encoder and decoder architectures, including BERT, DistilBERT, and Llama-2 using benchmarks such as GLUE, Squad, WikiText2.},
	urldate = {2025-04-15},
	publisher = {arXiv},
	author = {Flynn, Megan and Wang, Alexander and Alvarez, Dean Edward and Sa, Christopher De and Damle, Anil},
	month = may,
	year = {2024},
	note = {arXiv:2406.00061 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{park2025curing,
	title = {{CURing} {Large} {Models}: {Compression} via {CUR} {Decomposition}},
	shorttitle = {{CURing} {Large} {Models}},
	url = {http://arxiv.org/abs/2501.04211},
	doi = {10.48550/arXiv.2501.04211},
	abstract = {Large deep learning models have achieved remarkable success but are resource-intensive, posing challenges such as memory usage. We introduce CURing, a novel model compression method based on CUR matrix decomposition, which approximates weight matrices as the product of selected columns (C) and rows (R), and a small linking matrix (U). We apply this decomposition to weights chosen based on the combined influence of their magnitudes and activations. By identifying and retaining informative rows and columns, CURing significantly reduces model size with minimal performance loss. For example, it reduces Llama3.1-8B's parameters to 7.32B (-9\%) in just 129 seconds, over 20 times faster than prior compression methods.},
	urldate = {2025-04-15},
	publisher = {arXiv},
	author = {Park, Sanghyeon and Moon, Soo-Mook},
	month = jan,
	year = {2025},
	note = {arXiv:2501.04211 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{halko2011finding,
	title = {Finding {Structure} with {Randomness}: {Probabilistic} {Algorithms} for {Constructing} {Approximate} {Matrix} {Decompositions}},
	volume = {53},
	issn = {0036-1445, 1095-7200},
	shorttitle = {Finding {Structure} with {Randomness}},
	url = {http://epubs.siam.org/doi/10.1137/090771806},
	doi = {10.1137/090771806},
	language = {en},
	number = {2},
	urldate = {2025-01-24},
	journal = {SIAM Review},
	author = {Halko, N. and Martinsson, P. G. and Tropp, J. A.},
	month = jan,
	year = {2011},
	pages = {217--288},
}

@article{zheng2025semilagrangian,
	title = {A semi-{Lagrangian} adaptive-rank ({SLAR}) method for linear advection and nonlinear {Vlasov}-{Poisson} system},
	volume = {532},
	issn = {00219991},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999125002530},
	doi = {10.1016/j.jcp.2025.113970},
	language = {en},
	urldate = {2025-04-15},
	journal = {Journal of Computational Physics},
	author = {Zheng, Nanyi and Hayes, Daniel and Christlieb, Andrew and Qiu, Jing-Mei},
	month = jul,
	year = {2025},
	pages = {113970},
}

@article{civril2009selecting,
	title = {On selecting a maximum volume sub-matrix of a matrix and related problems},
	volume = {410},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {03043975},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304397509004101},
	doi = {10.1016/j.tcs.2009.06.018},
	language = {en},
	number = {47-49},
	urldate = {2025-04-15},
	journal = {Theoretical Computer Science},
	author = {Çivril, Ali and Magdon-Ismail, Malik},
	month = nov,
	year = {2009},
	pages = {4801--4811},
}

@article{yang2015identifying,
	title = {Identifying {Important} {Ions} and {Positions} in {Mass} {Spectrometry} {Imaging} {Data} {Using} {CUR} {Matrix} {Decompositions}},
	volume = {87},
	issn = {0003-2700, 1520-6882},
	url = {https://pubs.acs.org/doi/10.1021/ac5040264},
	doi = {10.1021/ac5040264},
	language = {en},
	number = {9},
	urldate = {2025-04-15},
	journal = {Analytical Chemistry},
	author = {Yang, Jiyan and Rübel, Oliver and {Prabhat} and Mahoney, Michael W. and Bowen, Benjamin P.},
	month = may,
	year = {2015},
	pages = {4658--4666},
}

@article{sorensen2016deim,
	title = {A {DEIM} {Induced} {CUR} {Factorization}},
	volume = {38},
	issn = {1064-8275, 1095-7197},
	url = {http://epubs.siam.org/doi/10.1137/140978430},
	doi = {10.1137/140978430},
	language = {en},
	number = {3},
	urldate = {2025-04-15},
	journal = {SIAM Journal on Scientific Computing},
	author = {Sorensen, D. C. and Embree, Mark},
	month = jan,
	year = {2016},
	pages = {A1454--A1482},
}

@article{meier2024fast,
	title = {Fast randomized numerical rank estimation for numerically low-rank matrices},
	volume = {686},
	issn = {00243795},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0024379524000016},
	doi = {10.1016/j.laa.2024.01.001},
	language = {en},
	urldate = {2025-04-15},
	journal = {Linear Algebra and its Applications},
	author = {Meier, Maike and Nakatsukasa, Yuji},
	month = apr,
	year = {2024},
	pages = {1--32},
}

@misc{dong2024robust,
	title = {Robust {Blockwise} {Random} {Pivoting}: {Fast} and {Accurate} {Adaptive} {Interpolative} {Decomposition}},
	shorttitle = {Robust {Blockwise} {Random} {Pivoting}},
	url = {http://arxiv.org/abs/2309.16002},
	doi = {10.48550/arXiv.2309.16002},
	abstract = {The interpolative decomposition (ID) aims to construct a low-rank approximation formed by a basis consisting of row/column skeletons in the original matrix and a corresponding interpolation matrix. This work explores fast and accurate ID algorithms from comprehensive perspectives for empirical performance, including accuracy in both skeleton selection and interpolation matrix construction, efficiency in terms of asymptotic complexity and hardware efficiency, as well as rank adaptiveness. While many algorithms have been developed to optimize some of these aspects, practical ID algorithms proficient in all aspects remain absent. To fill in the gap, we introduce robust blockwise random pivoting (RBRP) that is asymptotically fast, hardware-efficient, and rank-adaptive, providing accurate skeletons and interpolation matrices comparable to the best existing ID algorithms in practice. Through extensive numerical experiments on various synthetic and natural datasets, we demonstrate the appealing empirical performance of RBRP from the aforementioned perspectives, as well as the robustness of RBRP to adversarial inputs.},
	urldate = {2025-04-15},
	publisher = {arXiv},
	author = {Dong, Yijun and Chen, Chao and Martinsson, Per-Gunnar and Pearce, Katherine},
	month = dec,
	year = {2024},
	note = {arXiv:2309.16002 [math]},
	keywords = {Computer Science - Numerical Analysis, Mathematics - Numerical Analysis},
}

@article{buttari2008parallel,
	title = {Parallel tiled {QR} factorization for multicore architectures},
	volume = {20},
	copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
	issn = {1532-0626, 1532-0634},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/cpe.1301},
	doi = {10.1002/cpe.1301},
	abstract = {Abstract
            As multicore systems continue to gain ground in the high‐performance computing world, linear algebra algorithms have to be reformulated or new algorithms have to be developed in order to take advantage of the architectural features on these new processors. Fine‐grain parallelism becomes a major requirement and introduces the necessity of loose synchronization in the parallel execution of an operation. This paper presents an algorithm for the QR factorization where the operations can be represented as a sequence of small tasks that operate on square blocks of data (referred to as ‘tiles’). These tasks can be dynamically scheduled for execution based on the dependencies among them and on the availability of computational resources. This may result in an out‐of‐order execution of the tasks that will completely hide the presence of intrinsically sequential tasks in the factorization. Performance comparisons are presented with the LAPACK algorithm for QR factorization where parallelism can be exploited only at the level of the BLAS operations and with vendor implementations. Copyright © 2008 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {13},
	urldate = {2025-04-03},
	journal = {Concurrency and Computation: Practice and Experience},
	author = {Buttari, Alfredo and Langou, Julien and Kurzak, Jakub and Dongarra, Jack},
	month = sep,
	year = {2008},
	pages = {1573--1590},
}

@misc{cortinovis2024adaptive,
	title = {Adaptive randomized pivoting for column subset selection, {DEIM}, and low-rank approximation},
	url = {http://arxiv.org/abs/2412.13992},
	doi = {10.48550/arXiv.2412.13992},
	abstract = {We derive a new adaptive leverage score sampling strategy for solving the Column Subset Selection Problem (CSSP). The resulting algorithm, called Adaptive Randomized Pivoting, can be viewed as a randomization of Osinsky's recently proposed deterministic algorithm for CSSP. It guarantees, in expectation, an approximation error that matches the optimal existence result in the Frobenius norm. Although the same guarantee can be achieved with volume sampling, our sampling strategy is much simpler and less expensive. To show the versatility of Adaptive Randomized Pivoting, we apply it to select indices in the Discrete Empirical Interpolation Method, in cross/skeleton approximation of general matrices, and in the Nystroem approximation of symmetric positive semi-definite matrices. In all these cases, the resulting randomized algorithms are new and they enjoy bounds on the expected error that match -- or improve -- the best known deterministic results. A derandomization of the algorithm for the Nystroem approximation results in a new deterministic algorithm with a rather favorable error bound.},
	urldate = {2025-04-03},
	publisher = {arXiv},
	author = {Cortinovis, Alice and Kressner, Daniel},
	month = dec,
	year = {2024},
	note = {arXiv:2412.13992},
	keywords = {Computer Science - Numerical Analysis, Mathematics - Numerical Analysis},
}

@article{mitrovic2013cur,
	title = {{CUR} decomposition for compression and compressed sensing of large-scale traffic data},
	copyright = {Creative Commons Attribution-Noncommercial-Share Alike},
	url = {https://dspace.mit.edu/handle/1721.1/86879},
	abstract = {Intelligent Transportation Systems (ITS) often operate on large road networks, and typically collect traffic data with high temporal resolution. Consequently, ITS need to handle massive volumes of data, and methods to represent that data in more compact representations are sorely needed. Subspace methods such as Principal Component Analysis (PCA) can create accurate low-dimensional models. However, such models are not readily interpretable, as the principal components usually involve a large number of links in the traffic network. In contrast, the CUR matrix decomposition leads to low-dimensional models where the components correspond to individual links in the network; the resulting models can be easily interpreted, and can also be used for compressed sensing of the traffic network. In this paper, the CUR matrix decomposition is applied for two purposes: (1) compression of traffic data; (2) compressed sensing of traffic data. In the former, only data from a “random” subset of links and time instances is stored. In the latter, data for the entire traffic network is inferred from measurements at a “random” subset of links. Numerical results for a large traffic network in Singapore demonstrate the feasibility of the proposed approach.},
	language = {en\_US},
	urldate = {2025-04-03},
	journal = {MIT web domain},
	author = {Mitrovic, Nikola and Asif, Muhammad Tayyab and Rasheed, Umer and Dauwels, Justin and Jaillet, Patrick},
	month = oct,
	year = {2013},
}

@inproceedings{mai2020vgg,
	address = {Ho Chi Minh City, Vietnam},
	title = {{VGG} deep neural network compression via {SVD} and {CUR} decomposition techniques},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-0-7381-0553-6},
	url = {https://ieeexplore.ieee.org/document/9335842/},
	doi = {10.1109/NICS51282.2020.9335842},
	urldate = {2025-04-03},
	booktitle = {2020 7th {NAFOSTED} {Conference} on {Information} and {Computer} {Science} ({NICS})},
	publisher = {IEEE},
	author = {Mai, An and Tran, Loc and Tran, Linh and Trinh, Nguyen},
	month = nov,
	year = {2020},
	pages = {118--123},
}

@article{kramer2024learning,
	title = {Learning {Nonlinear} {Reduced} {Models} from {Data} with {Operator} {Inference}},
	volume = {56},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	issn = {0066-4189, 1545-4479},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-fluid-121021-025220},
	doi = {10.1146/annurev-fluid-121021-025220},
	abstract = {This review discusses Operator Inference, a nonintrusive reduced modeling approach that incorporates physical governing equations by defining a structured polynomial form for the reduced model, and then learns the corresponding reduced operators from simulated training data. The polynomial model form of Operator Inference is sufficiently expressive to cover a wide range of nonlinear dynamics found in fluid mechanics and other fields of science and engineering, while still providing efficient reduced model computations. The learning steps of Operator Inference are rooted in classical projection-based model reduction; thus, some of the rich theory of model reduction can be applied to models learned with Operator Inference. This connection to projection-based model reduction theory offers a pathway toward deriving error estimates and gaining insights to improve predictions. Furthermore, through formulations of Operator Inference that preserve Hamiltonian and other structures, important physical properties such as energy conservation can be guaranteed in the predictions of the reduced model beyond the training horizon. This review illustrates key computational steps of Operator Inference through a large-scale combustion example.},
	language = {en},
	number = {1},
	urldate = {2025-03-24},
	journal = {Annual Review of Fluid Mechanics},
	author = {Kramer, Boris and Peherstorfer, Benjamin and Willcox, Karen E.},
	month = jan,
	year = {2024},
	pages = {521--548},
}

@misc{osinsky2023close,
	title = {Close to optimal column approximations with a single {SVD}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2308.09068},
	doi = {10.48550/ARXIV.2308.09068},
	abstract = {The best column approximation in the Frobenius norm with \$r\$ columns has an error at most \${\textbackslash}sqrt\{r+1\}\$ times larger than the truncated singular value decomposition. Reaching this bound in practice involves either expensive random volume sampling or at least \$r\$ executions of singular value decomposition. In this paper it will be shown that the same column approximation bound can be reached with only a single SVD (which can also be replaced with approximate SVD). As a corollary, it will be shown how to find a highly nondegenerate submatrix in \$r\$ rows of size \$N\$ in just \$O(Nr{\textasciicircum}2)\$ operations, which mostly has the same properties as the maximum volume submatrix.},
	urldate = {2025-02-05},
	publisher = {arXiv},
	author = {Osinsky, Alexander},
	year = {2023},
	keywords = {FOS: Mathematics, Numerical Analysis (math.NA)},
}

@misc{melgaard2015gaussian,
	title = {Gaussian {Elimination} with {Randomized} {Complete} {Pivoting}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1511.08528},
	doi = {10.48550/ARXIV.1511.08528},
	abstract = {Gaussian elimination with partial pivoting (GEPP) has long been among the most widely used methods for computing the LU factorization of a given matrix. However, this method is also known to fail for matrices that induce large element growth during the factorization process. In this paper, we propose a new scheme, Gaussian elimination with randomized complete pivoting (GERCP) for the efficient and reliable LU factorization of a given matrix. GERCP satisfies GECP (Gaussian elimination with complete pivoting) style element growth bounds with high probability, yet costs only marginally higher than GEPP. Our numerical experimental results strongly suggest that GERCP is as reliable as GECP and as efficient as GEPP for computing the LU factorization.},
	urldate = {2025-01-14},
	publisher = {arXiv},
	author = {Melgaard, Christopher and Gu, Ming},
	year = {2015},
	keywords = {60, 65, Computation (stat.CO), FOS: Computer and information sciences, FOS: Mathematics, Numerical Analysis (math.NA)},
}

@misc{townsend2016gaussian,
	title = {Gaussian elimination corrects pivoting mistakes},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1602.06602},
	doi = {10.48550/ARXIV.1602.06602},
	abstract = {Gaussian elimination (GE) is the archetypal direct algorithm for solving linear systems of equations and this has been its primary application for thousands of years. In the last decade, GE has found another major use as an iterative algorithm for low rank approximation. In this setting, GE is often employed with complete pivoting and designed to allow for non-optimal pivoting, i.e., pivoting mistakes, that could render GE numerically unstable when implemented in floating point arithmetic. While it may appear that pivoting mistakes could accumulate and lead to a large growth factor, we show that later GE steps correct earlier pivoting mistakes, even while more are being made. In short, GE is very robust to non-optimal pivots, allowing for its iterative variant to flourish.},
	urldate = {2025-01-14},
	publisher = {arXiv},
	author = {Townsend, Alex},
	year = {2016},
	keywords = {FOS: Mathematics, Numerical Analysis (math.NA)},
}

@article{duersch2020randomized,
	title = {Randomized {Projection} for {Rank}-{Revealing} {Matrix} {Factorizations} and {Low}-{Rank} {Approximations}},
	volume = {62},
	issn = {0036-1445, 1095-7200},
	url = {https://epubs.siam.org/doi/10.1137/20M1335571},
	doi = {10.1137/20M1335571},
	language = {en},
	number = {3},
	urldate = {2025-01-14},
	journal = {SIAM Review},
	author = {Duersch, Jed A. and Gu, Ming},
	month = jan,
	year = {2020},
	pages = {661--682},
}

@book{stewart1998matrix,
	title = {Matrix {Algorithms}: {Volume} 1: {Basic} {Decompositions}},
	isbn = {978-0-89871-414-2 978-1-61197-140-8},
	shorttitle = {Matrix {Algorithms}},
	url = {http://epubs.siam.org/doi/book/10.1137/1.9781611971408},
	language = {en},
	urldate = {2025-02-19},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Stewart, G. W.},
	month = jan,
	year = {1998},
	doi = {10.1137/1.9781611971408},
}

@article{mahoney2009cur,
	title = {{CUR} matrix decompositions for improved data analysis},
	volume = {106},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.0803205106},
	doi = {10.1073/pnas.0803205106},
	abstract = {Principal components analysis and, more generally, the Singular Value Decomposition are fundamental data analysis tools that express a data matrix in terms of a sequence of orthogonal or uncorrelated vectors of decreasing importance. Unfortunately, being linear combinations of up to all the data points, these vectors are notoriously difficult to interpret in terms of the data and processes generating the data. In this article, we develop CUR matrix decompositions for improved data analysis. CUR decompositions are low-rank matrix decompositions that are explicitly expressed in terms of a small number of actual columns and/or actual rows of the data matrix. Because they are constructed from actual data elements, CUR decompositions are interpretable by practitioners of the field from which the data are drawn (to the extent that the original data are). We present an algorithm that preferentially chooses columns and rows that exhibit high “statistical leverage” and, thus, in a very precise statistical sense, exert a disproportionately large “influence” on the best low-rank fit of the data matrix. By selecting columns and rows in this manner, we obtain improved relative-error and constant-factor approximation guarantees in worst-case analysis, as opposed to the much coarser additive-error guarantees of prior work. In addition, since the construction involves computing quantities with a natural and widely studied statistical interpretation, we can leverage ideas from diagnostic regression analysis to employ these matrix decompositions for exploratory data analysis.},
	language = {en},
	number = {3},
	urldate = {2025-02-17},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Mahoney, Michael W. and Drineas, Petros},
	month = jan,
	year = {2009},
	pages = {697--702},
}

@article{yu2018efficient,
	title = {Efficient {Randomized} {Algorithms} for the {Fixed}-{Precision} {Low}-{Rank} {Matrix} {Approximation}},
	volume = {39},
	issn = {0895-4798, 1095-7162},
	url = {https://epubs.siam.org/doi/10.1137/17M1141977},
	doi = {10.1137/17M1141977},
	language = {en},
	number = {3},
	urldate = {2025-02-11},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Yu, Wenjian and Gu, Yu and Li, Yaohang},
	month = jan,
	year = {2018},
	pages = {1339--1359},
}

@article{pilanci2016iterative,
	title = {Iterative {Hessian} {Sketch}: {Fast} and {Accurate} {Solution} {Approximation} for {Constrained} {Least}-{Squares}},
	volume = {17},
	issn = {1533-7928},
	shorttitle = {Iterative {Hessian} {Sketch}},
	url = {http://jmlr.org/papers/v17/14-460.html},
	abstract = {We study randomized sketching methods for approximately solving least-squares problem with a general convex constraint. The quality of a least-squares approximation can be assessed in different ways: either in terms of the value of the quadratic objective function (cost approximation), or in terms of some distance measure between the approximate minimizer and the true minimizer (solution approximation). Focusing on the latter criterion, our first main result provides a general lower bound on any randomized method that sketches both the data matrix and vector in a least-squares problem; as a surprising consequence, the most widely used least-squares sketch is sub-optimal for solution approximation. We then present a new method known as the iterative Hessian sketch, and show that it can be used to obtain approximations to the original least-squares problem using a projection dimension proportional to the statistical complexity of the least-squares minimizer, and a logarithmic number of iterations. We illustrate our general theory with simulations for both unconstrained and constrained versions of least-squares, including \${\textbackslash}ell\_1\$-regularization and nuclear norm constraints. We also numerically demonstrate the practicality of our approach in a real face expression classification experiment.},
	number = {53},
	urldate = {2025-02-11},
	journal = {Journal of Machine Learning Research},
	author = {Pilanci, Mert and Wainwright, Martin J.},
	year = {2016},
	pages = {1--38},
}

@inproceedings{chen2020efficient,
	title = {Efficient {Spectrum}-{Revealing} {CUR} {Matrix} {Decomposition}},
	url = {https://proceedings.mlr.press/v108/chen20a.html},
	abstract = {The CUR matrix decomposition is an important tool for low-rank matrix approximation. It approximates a data matrix though selecting a small number of columns and rows of the matrix. Those CUR algorithms with  gap-dependent approximation bounds can obtain high approximation quality for matrices with good singular value spectrum decay, but they have impractically high time complexities. In this paper, we propose a novel CUR algorithm based on truncated LU factorization with an efficient variant of complete pivoting. Our algorithm has gap-dependent approximation bounds on both spectral and Frobenius norms while maintaining high efficiency. Numerical experiments demonstrate the effectiveness of our algorithm and verify our theoretical guarantees.},
	language = {en},
	urldate = {2025-02-05},
	booktitle = {Proceedings of the {Twenty} {Third} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Chen, Cheng and Gu, Ming and Zhang, Zhihua and Zhang, Weinan and Yu, Yong},
	month = jun,
	year = {2020},
	pages = {766--775},
}

@article{anderson2016efficient,
	title = {An {Efficient}, {Sparsity}-{Preserving}, {Online} {Algorithm} for {Low}-{Rank} {Approximation}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1602.05950},
	doi = {10.48550/ARXIV.1602.05950},
	abstract = {Low-rank matrix approximation is a fundamental tool in data analysis for processing large datasets, reducing noise, and finding important signals. In this work, we present a novel truncated LU factorization called Spectrum-Revealing LU (SRLU) for effective low-rank matrix approximation, and develop a fast algorithm to compute an SRLU factorization. We provide both matrix and singular value approximation error bounds for the SRLU approximation computed by our algorithm. Our analysis suggests that SRLU is competitive with the best low-rank matrix approximation methods, deterministic or randomized, in both computational complexity and approximation quality. Numeric experiments illustrate that SRLU preserves sparsity, highlights important data features and variables, can be efficiently updated, and calculates data approximations nearly as accurately as possible. To the best of our knowledge this is the first practical variant of the LU factorization for effective and efficient low-rank matrix approximation.},
	urldate = {2025-01-14},
	author = {Anderson, David G. and Gu, Ming},
	year = {2016},
	keywords = {FOS: Mathematics, Numerical Analysis (math.NA)},
}

@article{dong2023simpler,
	title = {Simpler is better: a comparative study of randomized pivoting algorithms for {CUR} and interpolative decompositions},
	volume = {49},
	issn = {1019-7168, 1572-9044},
	shorttitle = {Simpler is better},
	url = {https://link.springer.com/10.1007/s10444-023-10061-z},
	doi = {10.1007/s10444-023-10061-z},
	language = {en},
	number = {4},
	urldate = {2025-02-05},
	journal = {Advances in Computational Mathematics},
	author = {Dong, Yijun and Martinsson, Per-Gunnar},
	month = aug,
	year = {2023},
	pages = {66},
}

@article{udell2019why,
	title = {Why {Are} {Big} {Data} {Matrices} {Approximately} {Low} {Rank}?},
	volume = {1},
	issn = {2577-0187},
	url = {https://epubs.siam.org/doi/10.1137/18M1183480},
	doi = {10.1137/18M1183480},
	language = {en},
	number = {1},
	urldate = {2025-02-05},
	journal = {SIAM Journal on Mathematics of Data Science},
	author = {Udell, Madeleine and Townsend, Alex},
	month = jan,
	year = {2019},
	pages = {144--160},
}
