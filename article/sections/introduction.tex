For decades, low-rank approximations have been essential for the scalability improvements achieved in a variety of scientific computing fields, 
 ranging from mass spectrometry \cite{yang2015identifying}, surrogate modeling \cite{kramer2024learning,zheng2025semilagrangian}, genetics \cite{mahoney2009cur}, and deep learning \cite{flynn2024stat, mai2020vgg, park2025curing}.  In these fields, practitioners often form low-rank approximations using optimal SVD-based  approaches such as the truncated SVD \cite{eckart1936approximation, mirsky1960symmetric} and the RandomizedSVD \cite{halko2011finding}.  However, while some SVD-based methods are very computationally efficient, it turns out that performance can be further improved by switching to alternative low-rank approximation methodologies such as the CUR decomposition, which has the additional advantage that it facilitates data interpretation \cite{mahoney2009cur}. 

The CUR decomposition \cite{goreinov1997theory, mahoney2009cur} aims to find a rank-$r$ approximation to a matrix $\mat A \in \real^{m \times n}$ by choosing columns of $\mat A$, $\mat C \in \real^{m \times r}$, rows of $\mat A$, $\mat R \in \real^{r \times n}$, and a 
core matrix $\mat U \in \real^{r \times r}$ such that 
\begin{equation}
    \mat A \approx \mat C \mat U \mat R.
\end{equation}

The CUR decomposition offers superior scalability and better preservation of sparsity and non-negativity compared to SVD-based methods because it works directly with selected subsets of the original matrix's rows and columns. However, this advantage comes with a trade-off: CUR approximations typically achieve somewhat lower accuracy than their SVD counterparts. This accuracy limitation stems from the constraint that the $\mat C$ and $\mat R$ matrices must consist of actual column and row subsets from the original matrix, preventing CUR from reaching the optimal accuracy that a truncated SVD of the same rank can achieve.
Nonetheless, incredibly, the CUR factorization will be at worst within a factor of $r+1$ away from the optimal SVD-based rank-$r$ approximation in the Frobenius norm, provided that optimal rows and columns are chosen~\cite{cortinovis2020lowrank,deshpande2006no,osinsky2025close}\footnote{This is for the "efficient" choice of $\mat U$, 
$\mat{U}=\mat{A}(I,J)^{\dagger}$, sometimes called CUR cross approximation~\cite{park2025accuracy}. 
One can improve the suboptimality factor to $\sqrt{2(r+1)}$~\cite{cortinovis2020lowrank} with $\mat{U}=\mat{C}^\dagger \mat{AR}^\dagger$ (CUR best approximation), which minimizes the Frobenius norm error; but computing this is expensive, and we make little use of it in this paper. }. 



Selecting rows and columns that \textit{optimally} minimize the Frobenius norm of the error is NP-Hard \cite{shitov2021column, civril2009selecting}. Instead, most CUR index selection techniques aim to guarantee high-quality approximations to a matrix. Broadly, these selection techniques can be categorized as sampling techniques, pivoting techniques, or pivoting on a subspace techniques. Sampling techniques select indices by sampling from fixed distributions (e.g., Uniform, Leverage Score, DPP) over the row and column indices \cite{derezinski2021determinantal, frieze2004fast, mahoney2009cur}.  Pivoting techniques select indices by applying a pivoting procedure directly to the matrix (e.g., LU with partial pivoting and QR with column pivoting) \cite{stewart1998matrix}. Pivoting on subspace techniques work by applying a pivoting technique to a subspace of the matrix, typically obtained via the truncated SVD or through matrix sketching. These techniques offer some of the best approximation performance and computational efficiency in practice, but are often more difficult to analyze theoretically \cite{dong2023simpler}. Examples of pivoting on a subspace techniques include pivoting on a sketch techniques such as sketched LU with partial pivoting \cite{dong2023simpler}, DEIM \cite{sorensen2016deim}, Osinsky's selection approach \cite{osinsky2025close} as well as many others \cite{chen2020efficient, cortinovis2024adaptive,duersch2020randomized, osinsky2025close}. Beyond these categories, there are many hybrid approaches, which often involve choosing pivots via sampling that include techniques such as robust block-wise random pivoting \cite{dong2024robust} or RP-Cholesky \cite{chen2025randomly}. 

Typically, index-selection techniques require that the rank be known before selecting the indices \cite{chen2020efficient, cortinovis2024adaptive, dong2023simpler, duersch2020randomized, mahoney2009cur}.  Unfortunately, in many applications, practitioners do not have this knowledge \cite{kramer2024learning, park2025curing, yang2015identifying}. Instead, practitioners often prefer that the approximation, $\widehat{\mat A}$, satisfies a particular quality threshold, for example, the relative error, $\|\mat A - \widehat {\mat A}\|_F / \|\mat A\|_F < \epsilon$. Current CUR approaches can approximate this behavior using a rank estimation procedure such as the one proposed in \cite{meier2024fast} that uses estimates of singular values to determine a rank that corresponds to the desired quality threshold. Unfortunately, owing to the sub-optimality of the CUR for the best rank-$r$ approximation to a matrix, this approach does not guarantee that the returned CUR approximation will satisfy the desired quality threshold.

Guaranteeing quality requires a rank-adaptive CUR approach. Rank-adaptive approaches iteratively increase the rank of an approximation until a particular quality threshold is satisfied. As an example, a rank-adaptive variant of RandomizedSVD \cite{halko2011finding} known as SVDSketch \cite{yu2018efficient} iteratively updates the range approximator, $\mat Q$, with sketches of size $b$ of $\mat A$ until $\|\mat A\|_F^2 - \|\mat Q^\top\mat A\|_F^2$ is small enough. Beyond RandomizedSVD, several innovative and highly effective rank-adaptive approaches have been proposed for one-sided interpolatory decompositions, which are closely related to CUR but select \textit{either} rows \textit{or} columns (not both) \cite{dong2024robust, pearce2025adaptive}.


This work introduces a rank-adaptive CUR framework, IterativeCUR, that requires only $\mathcal{O}(r(m+n))$ memory to update the approximation with the same computational complexity, $\mathcal{O}(mn+((m+n)r^2))$, as other state-of-the-art CUR approaches \cite{dong2023simpler}. IterativeCUR forms a CUR approximation by selecting indices from a single recycled sketch of the residual, $\mat A - \mat C \mat U\mat R$, that holds $\mathcal{O}(1)$ vectors. This single sketch allows IterativeCUR to thrive in environments where matrix-vector operations are expensive, e.g., when the matrix $\mat A$ is very large and dense. The sketch of the residual also facilitates the creation of a risk-aware stopping criterion for IterativeCUR, which allows users to control the probability that the approximation returned by IterativeCUR violates their quality threshold. 
 In experiments, we demonstrate the effectiveness of IterativeCUR on a range of moderately sized real-world and synthetic matrices.  Specifically, we observed that IterativeCUR is four times faster than the state-of-the-art column selection procedure, Sketched LUPP \cite{dong2023simpler} (which is not rank-adaptive), while matching its accuracy. Additionally, compared to the rank-adaptive approach for the RandomizedSVD, SVDSketch, IterativeCUR is typically nearly as accurate and up to 40 times faster.  Experiments examining IterativeCUR revealed that high accuracy is maintained even for block sizes as small as five. However, for optimal computational speed, larger block sizes (around 100) perform better. The experiments further indicate that while the choice of index selection strategy does have an impact on accuracy, the differences are small. For more details on the experiments, see \cref{sec:exp}. 

Simply put, this paper introduces an efficient and practical method for computing CUR approximations that substantially outperforms RandomizedSVD. The approach achieves this efficiency through two key features: it requires remarkably few matrix-vector products with the full matrix (as few as five), and its rank-adaptive design automatically determines the optimal rank to achieve  $\epsilon$-approximations with  $\epsilon$ approaching machine precision. By contrast, SVDsketch is unable to obtain an accuracy higher than square root of the machine precision~\cite{yu2018efficient}.


In the next subsection, we introduce the relevant notation for the paper.  After this notation is introduced, the paper will proceed in the following manner. 
In \cref{sec:pre}, we present background relating to the CUR decomposition and random embeddings that is relevant for understanding IterativeCUR.
In \cref{sec:alg}, we present the intuition, describe the specifics, and explore the computational complexity of IterativeCUR. 
In \cref{sec:the}, we present an upper bound on the accuracy of IterativeCUR.
In \cref{sec:exp}, we present extensive numerical experiments that demonstrate the efficiency of row and column selection compared to other fixed-rank approaches, the overall effectiveness of IterativeCUR when identifying an approximation for a specified error tolerance, and the impact of sketch size and index selection method on the performance of IterativeCUR. Finally, in \cref{sec:con}, we conclude with a summary of IterativeCUR's practical advantages and remaining theoretical challenges.
